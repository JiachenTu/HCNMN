# V20 Vision-Grounded VLM Method Report
======================================================================

## Method Overview

**V20 Vision-Grounded VLM-Guided Granularity Selection**

Key Features:
- Vision-Language Model: Qwen2.5-VL-3B-Instruct
- Visual Grounding: VLM sees actual image
- Scene Context: Full scene graph (objects + relationships + bboxes)
- Two-Stage Selection: Mid-level first, then coarse-level
- Vision-Aware Reasoning: Considers visual features for semantic decisions
- Fallback: V7 adaptive rules if VLM fails

## Performance Statistics

Total objects processed: 14
VLM success: 3/14 (21.4%)
Fallback to V7 rules: 5/14 (35.7%)

## Vision-Grounded Reasoning Examples

Sample VLM reasoning with visual grounding:

1. **man** → person (mid) → object (coarse)
   Reasoning: A person wearing a life jacket standing on the beach.

2. **surfboard** → artifact (mid) → object (coarse)
   Reasoning: The surfboard is an artifact made of material, placed on the beach.

3. **skirt** → artifact (mid) → object (coarse)
   Reasoning: Woman wearing skirt standing near blue surfboard.

## Comparison with V8.1

V8.1 (Text-Only LLM):
- Uses Qwen3-0.6B text-only LLM
- Scene context: text descriptions only
- No visual grounding

V20 (Vision-Language Model):
- Uses Qwen2.5-VL-3B vision-language model
- Scene context: text + actual image
- Visual grounding: VLM can see objects and their visual features
- Expected improvement: Better semantic decisions for visually-ambiguous objects

