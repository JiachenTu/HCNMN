# V20 Vision-Grounded VLM Method Report
======================================================================

## Method Overview

**V20 Vision-Grounded VLM-Guided Granularity Selection**

Key Features:
- Vision-Language Model: Qwen2.5-VL-3B-Instruct
- Visual Grounding: VLM sees actual image
- Scene Context: Full scene graph (objects + relationships + bboxes)
- Two-Stage Selection: Mid-level first, then coarse-level
- Vision-Aware Reasoning: Considers visual features for semantic decisions
- Fallback: V7 adaptive rules if VLM fails

## Performance Statistics

Total objects processed: 9
VLM success: 6/9 (66.7%)
Fallback to V7 rules: 2/9 (22.2%)

## Vision-Grounded Reasoning Examples

Sample VLM reasoning with visual grounding:

1. **building** → artifact (mid) → object (coarse)
   Reasoning: Building visible as a constructed structure with multiple windows and a facade.

2. **emblem** → artifact (mid) → object (coarse)
   Reasoning: Circular emblem mounted on a pole

3. **window** → artifact (mid) → object (coarse)
   Reasoning: Window on a building visible in the image

4. **sidewalk** → artifact (mid) → object (coarse)
   Reasoning: The sidewalk is a constructed surface used for walking.

5. **steps** → artifact (mid) → object (coarse)
   Reasoning: Steps visible as part of urban infrastructure

6. **light** → natural phenomenon (mid) → phenomenon (coarse)
   Reasoning: Light is an electromagnetic radiation that appears as a natural phenomenon.

## Comparison with V8.1

V8.1 (Text-Only LLM):
- Uses Qwen3-0.6B text-only LLM
- Scene context: text descriptions only
- No visual grounding

V20 (Vision-Language Model):
- Uses Qwen2.5-VL-3B vision-language model
- Scene context: text + actual image
- Visual grounding: VLM can see objects and their visual features
- Expected improvement: Better semantic decisions for visually-ambiguous objects

