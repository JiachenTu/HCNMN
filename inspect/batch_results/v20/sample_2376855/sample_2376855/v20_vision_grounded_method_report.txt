# V20 Vision-Grounded VLM Method Report
======================================================================

## Method Overview

**V20 Vision-Grounded VLM-Guided Granularity Selection**

Key Features:
- Vision-Language Model: Qwen2.5-VL-3B-Instruct
- Visual Grounding: VLM sees actual image
- Scene Context: Full scene graph (objects + relationships + bboxes)
- Two-Stage Selection: Mid-level first, then coarse-level
- Vision-Aware Reasoning: Considers visual features for semantic decisions
- Fallback: V7 adaptive rules if VLM fails

## Performance Statistics

Total objects processed: 23
VLM success: 10/23 (43.5%)
Fallback to V7 rules: 9/23 (39.1%)

## Vision-Grounded Reasoning Examples

Sample VLM reasoning with visual grounding:

1. **ear** → organ (mid) → physical entity (coarse)
   Reasoning: Ear is a part of the head that is used for hearing.

2. **cellphone** → artifact (mid) → physical entity (coarse)
   Reasoning: Visible electronic device used for communication

3. **fingernail** → part (mid) → thing (coarse)
   Reasoning: Fingernail is a part of the hand, which is a thing.

4. **man** → person (mid) → whole (coarse)
   Reasoning: Man is a living organism and a whole entity, represented by his head, face, and body.

5. **speaker** → organism (mid) → object (coarse)
   Reasoning: The speaker is part of a larger electronic device, which is an organism.

6. **buttons** → artifact (mid) → object (coarse)
   Reasoning: Buttons are visible as part of the cell phone's interface.

7. **watch** → artifact (mid) → object (coarse)
   Reasoning: Visible watch with a strap in hand

8. **keypad** → instrumentality (mid) → object (coarse)
   Reasoning: Visible keypad part of a cellular phone

9. **latch** → artifact (mid) → object (coarse)
   Reasoning: Visible metallic latch attached to a door or wall

10. **picture** → artifact (mid) → object (coarse)
   Reasoning: Visible photograph or digital image displayed on the phone's screen

## Comparison with V8.1

V8.1 (Text-Only LLM):
- Uses Qwen3-0.6B text-only LLM
- Scene context: text descriptions only
- No visual grounding

V20 (Vision-Language Model):
- Uses Qwen2.5-VL-3B vision-language model
- Scene context: text + actual image
- Visual grounding: VLM can see objects and their visual features
- Expected improvement: Better semantic decisions for visually-ambiguous objects

