# V20 Vision-Grounded VLM Method Report
======================================================================

## Method Overview

**V20 Vision-Grounded VLM-Guided Granularity Selection**

Key Features:
- Vision-Language Model: Qwen2.5-VL-3B-Instruct
- Visual Grounding: VLM sees actual image
- Scene Context: Full scene graph (objects + relationships + bboxes)
- Two-Stage Selection: Mid-level first, then coarse-level
- Vision-Aware Reasoning: Considers visual features for semantic decisions
- Fallback: V7 adaptive rules if VLM fails

## Performance Statistics

Total objects processed: 14
VLM success: 5/14 (35.7%)
Fallback to V7 rules: 3/14 (21.4%)

## Vision-Grounded Reasoning Examples

Sample VLM reasoning with visual grounding:

1. **surfboard** → artifact (mid) → object (coarse)
   Reasoning: Visible surfboard with blue color and white text

2. **man** → person (mid) → object (coarse)
   Reasoning: Man wearing red shirt and shorts standing on the beach

3. **skirt** → artifact (mid) → object (coarse)
   Reasoning: The skirt is an object that is part of the overall whole.

4. **woman** → person (mid) → physical entity (coarse)
   Reasoning: Woman wearing skirt and glasses, standing on the beach

5. **beach** → beach (mid) → physical entity (coarse)
   Reasoning: Visible sandy area with people and objects

## Comparison with V8.1

V8.1 (Text-Only LLM):
- Uses Qwen3-0.6B text-only LLM
- Scene context: text descriptions only
- No visual grounding

V20 (Vision-Language Model):
- Uses Qwen2.5-VL-3B vision-language model
- Scene context: text + actual image
- Visual grounding: VLM can see objects and their visual features
- Expected improvement: Better semantic decisions for visually-ambiguous objects

