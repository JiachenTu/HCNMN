# V20 Vision-Grounded VLM Method Report
======================================================================

## Method Overview

**V20 Vision-Grounded VLM-Guided Granularity Selection**

Key Features:
- Vision-Language Model: Qwen2.5-VL-3B-Instruct
- Visual Grounding: VLM sees actual image
- Scene Context: Full scene graph (objects + relationships + bboxes)
- Two-Stage Selection: Mid-level first, then coarse-level
- Vision-Aware Reasoning: Considers visual features for semantic decisions
- Fallback: V7 adaptive rules if VLM fails

## Performance Statistics

Total objects processed: 22
VLM success: 7/22 (31.8%)
Fallback to V7 rules: 11/22 (50.0%)

## Vision-Grounded Reasoning Examples

Sample VLM reasoning with visual grounding:

1. **man** → person (mid) → object (coarse)
   Reasoning: Visible human figure wearing blue shirt

2. **bag** → artifact (mid) → object (coarse)
   Reasoning: Bag is a piece of equipment used for carrying items.

3. **tail** → part (mid) → thing (coarse)
   Reasoning: Tail is a part of the kite, which is a thing.

4. **building** → artifact (mid) → object (coarse)
   Reasoning: Building visible as a constructed structure in the background

5. **cones** → artifact (mid) → object (coarse)
   Reasoning: Visible cone-shaped objects on the ground

6. **woman** → person (mid) → physical entity (coarse)
   Reasoning: Woman is a living being

7. **boy** → person (mid) → object (coarse)
   Reasoning: Boy is a person standing on the grass.

## Comparison with V8.1

V8.1 (Text-Only LLM):
- Uses Qwen3-0.6B text-only LLM
- Scene context: text descriptions only
- No visual grounding

V20 (Vision-Language Model):
- Uses Qwen2.5-VL-3B vision-language model
- Scene context: text + actual image
- Visual grounding: VLM can see objects and their visual features
- Expected improvement: Better semantic decisions for visually-ambiguous objects

